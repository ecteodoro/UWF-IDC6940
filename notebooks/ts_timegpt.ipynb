{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# TimeGPT-style Transformer forecaster (smoke notebook)\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook creates a small Transformer-based forecaster that mirrors the N-BEATS flow but uses a lightweight TimeGPT-like architecture implemented with PyTorch Lightning. It's intended as a smoke-test: small model, few steps.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"import importlib, os, sys\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import pandas as pd, numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from torch import nn\\n\",\n",
    "    \"import pytorch_lightning as pl\\n\",\n",
    "    \"print('Python', sys.version)\\n\",\n",
    "    \"print('Torch', torch.__version__)\\n\",\n",
    "    \"# repo root and data path\\n\",\n",
    "    \"ROOT = Path('..').resolve()\\n\",\n",
    "    \"DATA_PATH = ROOT / 'data' / 'cherry_blossom_data.csv'\\n\",\n",
    "    \"print('Using data file:', DATA_PATH)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# load data and minimal preprocessing (same as other notebooks)\\n\",\n",
    "    \"df = pd.read_csv(DATA_PATH)\\n\",\n",
    "    \"numeric_cols = ['days_dec_ge_45','days_jan_ge_45','days_feb_ge_45','prec_winter','mean_temp_winter','surface_temp_chg','climate_incidents']\\n\",\n",
    "    \"for c in numeric_cols + ['bloom_day','year']:\\n\",\n",
    "    \"    df[c] = pd.to_numeric(df[c], errors='coerce')\\n\",\n",
    "    \"df = df.sort_values('year').drop_duplicates('year').dropna(subset=['year','bloom_day']).reset_index(drop=True)\\n\",\n",
    "    \"df['time_idx'] = (df['year'] - df['year'].min()).astype(int)\\n\",\n",
    "    \"df['series'] = 'cherry_blossom'\\n\",\n",
    "    \"train_val_df = df[(df['year'] >= 1921) & (df['year'] <= 2015)].sort_values('time_idx').reset_index(drop=True)\\n\",\n",
    "    \"train_n = int(np.floor(len(train_val_df)*0.8))\\n\",\n",
    "    \"train_df = train_val_df.iloc[:train_n].reset_index(drop=True)\\n\",\n",
    "    \"val_df = train_val_df.iloc[train_n:].reset_index(drop=True)\\n\",\n",
    "    \"test_df = df[(df['year'] >= 2016) & (df['year'] <= 2025)].sort_values('time_idx').reset_index(drop=True)\\n\",\n",
    "    \"print('Train/Val/Test:', len(train_df), len(val_df), len(test_df))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# build TimeSeriesDataSet (univariate target only for TimeGPT simplicity)\\n\",\n",
    "    \"pf = importlib.import_module('pytorch_forecasting')\\n\",\n",
    "    \"TimeSeriesDataSet = getattr(pf, 'TimeSeriesDataSet')\\n\",\n",
    "    \"pf_data = importlib.import_module('pytorch_forecasting.data')\\n\",\n",
    "    \"GroupNormalizer = getattr(pf_data, 'GroupNormalizer')\\n\",\n",
    "    \"TARGET = 'bloom_day'\\n\",\n",
    "    \"max_encoder_length = min(30, len(train_df))\\n\",\n",
    "    \"fixed_encoder_length = max(1, max_encoder_length)\\n\",\n",
    "    \"fixed_prediction_length = 1\\n\",\n",
    "    \"training = TimeSeriesDataSet(\\n\",\n",
    "    \"    train_df,\\n\",\n",
    "    \"    time_idx='time_idx',\\n\",\n",
    "    \"    target=TARGET,\\n\",\n",
    "    \"    group_ids=['series'],\\n\",\n",
    "    \"    min_encoder_length=fixed_encoder_length,\\n\",\n",
    "    \"    max_encoder_length=fixed_encoder_length,\\n\",\n",
    "    \"    min_prediction_length=fixed_prediction_length,\\n\",\n",
    "    \"    max_prediction_length=fixed_prediction_length,\\n\",\n",
    "    \"    time_varying_known_reals=[],\\n\",\n",
    "    \"    time_varying_unknown_reals=[TARGET],\\n\",\n",
    "    \"    time_varying_known_categoricals=[],\\n\",\n",
    "    \"    time_varying_unknown_categoricals=[],\\n\",\n",
    "    \"    static_categoricals=[],\\n\",\n",
    "    \"    static_reals=[],\\n\",\n",
    "    \"    add_relative_time_idx=False,\\n\",\n",
    "    \"    target_normalizer=GroupNormalizer(groups=['series']),\\n\",\n",
    "    \"    randomize_length=None,\\n\",\n",
    "    \")\\n\",\n",
    "    \"# validation dataset: prepend small history if needed\\n\",\n",
    "    \"pre_history_len = int(training.max_encoder_length)\\n\",\n",
    "    \"if len(train_df) >= pre_history_len and len(val_df) < pre_history_len + 1:\\n\",\n",
    "    \"    pre_history = train_df.tail(pre_history_len).copy()\\n\",\n",
    "    \"    val_with_history = pd.concat([pre_history, val_df], ignore_index=True)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    val_with_history = val_df.copy()\\n\",\n",
    "    \"validation = TimeSeriesDataSet.from_dataset(\\n\",\n",
    "    \"    training, val_with_history, stop_randomization=True, allow_missing_timesteps=True,\\n\",\n",
    "    \"    min_encoder_length=training.max_encoder_length, max_encoder_length=training.max_encoder_length,\\n\",\n",
    "    \"    min_prediction_length=training.max_prediction_length, max_prediction_length=training.max_prediction_length, randomize_length=None\\n\",\n",
    "    \")\\n\",\n",
    "    \"train_dataloader = training.to_dataloader(train=True, batch_size=16, num_workers=0)\\n\",\n",
    "    \"val_dataloader = validation.to_dataloader(train=False, batch_size=16, num_workers=0)\\n\",\n",
    "    \"print('Created dataloaders; sample batch types:')\\n\",\n",
    "    \"batch = next(iter(train_dataloader))\\n\",\n",
    "    \"print(type(batch))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# define a tiny Transformer-based forecaster (TimeGPT-like smoke model)\\n\",\n",
    "    \"class SmallTimeGPT(pl.LightningModule):\\n\",\n",
    "    \"    def __init__(self, enc_len, pred_len=1, d_model=32, nhead=4, num_layers=2, lr=1e-3):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.save_hyperparameters()\\n\",\n",
    "    \"        self.enc_len = enc_len\\n\",\n",
    "    \"        self.pred_len = pred_len\\n\",\n",
    "    \"        self.d_model = d_model\\n\",\n",
    "    \"        # project scalar target -> d_model\\n\",\n",
    "    \"        self.in_proj = nn.Linear(1, d_model)\\n\",\n",
    "    \"        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=128, batch_first=True)\\n\",\n",
    "    \"        self.transformer_enc = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\\n\",\n",
    "    \"        # use pooled representation to predict future steps\\n\",\n",
    "    \"        self.out = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, pred_len))\\n\",\n",
    "    \"        self.loss_fn = nn.MSELoss()\\n\",\n",
    "    \"        self.lr = lr\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, encoder_values):\\n\",\n",
    "    \"        # encoder_values: [B, enc_len] -> reshape to [B, enc_len, 1]\\n\",\n",
    "    \"        x = encoder_values.unsqueeze(-1).float()\\n\",\n",
    "    \"        x = self.in_proj(x)  # [B, enc_len, d_model]\\n\",\n",
    "    \"        x = self.transformer_enc(x)  # [B, enc_len, d_model]\\n\",\n",
    "    \"        # pool (mean over time)\\n\",\n",
    "    \"        pooled = x.mean(dim=1)  # [B, d_model]\\n\",\n",
    "    \"        out = self.out(pooled)  # [B, pred_len]\\n\",\n",
    "    \"        return out\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def training_step(self, batch, batch_idx):\\n\",\n",
    "    \"        # batch may be tuple (x,y) or dict; adapt\\n\",\n",
    "    \"        if isinstance(batch, (list, tuple)):\\n\",\n",
    "    \"            x = batch[0]  # dict-like\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            x = batch\\n\",\n",
    "    \"        enc = x['encoder_target']  # [B, enc_len]\\n\",\n",
    "    \"        dec = x.get('decoder_target', None)\\n\",\n",
    "    \"        if dec is None and 'decoder_target' in x:\\n\",\n",
    "    \"            dec = x['decoder_target']\\n\",\n",
    "    \"        # some PF versions put decoder_target in batch[1] when tuple provided\\n\",\n",
    "    \"        if dec is None and isinstance(batch, (list, tuple)) and len(batch) > 1:\\n\",\n",
    "    \"            dec = batch[1]\\n\",\n",
    "    \"        if dec is None:\\n\",\n",
    "    \"            # fallback: use last time step as target (not ideal but keeps smoke test running)\\n\",\n",
    "    \"            dec = enc[:, -1:].float()\\n\",\n",
    "    \"        pred = self(enc)\\n\",\n",
    "    \"        loss = self.loss_fn(pred.squeeze(-1) if pred.ndim==3 else pred, dec.float())\\n\",\n",
    "    \"        self.log('train_loss', loss, prog_bar=True)\\n\",\n",
    "    \"        return loss\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def validation_step(self, batch, batch_idx):\\n\",\n",
    "    \"        if isinstance(batch, (list, tuple)):\\n\",\n",
    "    \"            x = batch[0]\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            x = batch\\n\",\n",
    "    \"        enc = x['encoder_target']\\n\",\n",
    "    \"        dec = x.get('decoder_target', None)\\n\",\n",
    "    \"        if dec is None and isinstance(batch, (list, tuple)) and len(batch) > 1:\\n\",\n",
    "    \"            dec = batch[1]\\n\",\n",
    "    \"        if dec is None:\\n\",\n",
    "    \"            dec = enc[:, -1:].float()\\n\",\n",
    "    \"        pred = self(enc)\\n\",\n",
    "    \"        loss = self.loss_fn(pred.squeeze(-1) if pred.ndim==3 else pred, dec.float())\\n\",\n",
    "    \"        self.log('val_loss', loss, prog_bar=True)\\n\",\n",
    "    \"        return loss\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def configure_optimizers(self):\\n\",\n",
    "    \"        return torch.optim.Adam(self.parameters(), lr=self.lr)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# instantiate and run a short smoke training run\\n\",\n",
    "    \"enc_len = int(training.max_encoder_length)\\n\",\n",
    "    \"pred_len = int(training.max_prediction_length)\\n\",\n",
    "    \"model = SmallTimeGPT(enc_len=enc_len, pred_len=pred_len, d_model=32, nhead=4, num_layers=1, lr=1e-3)\\n\",\n",
    "    \"print(model)\\n\",\n",
    "    \"# small trainer for smoke test\\n\",\n",
    "    \"early_stop = pl.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')\\n\",\n",
    "    \"ckpt = pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1)\\n\",\n",
    "    \"trainer = pl.Trainer(max_epochs=3, devices=1 if torch.cuda.is_available() or torch.backends.mps.is_available() else None, accelerator='auto', limit_train_batches=10, limit_val_batches=5, callbacks=[early_stop, ckpt], logger=False, enable_progress_bar=False)\\n\",\n",
    "    \"trainer.fit(model, train_dataloader, val_dataloader)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# quick one-step-ahead predict on test set (sliding)\\n\",\n",
    "    \"preds = []\\n\",\n",
    "    \"for _, row in test_df.iterrows():\\n\",\n",
    "    \"    time_idx = int(row['time_idx'])\\n\",\n",
    "    \"    enc_end = time_idx - 1\\n\",\n",
    "    \"    enc_start = max(0, enc_end - enc_len + 1)\\n\",\n",
    "    \"    enc_df = df[(df['time_idx'] >= enc_start) & (df['time_idx'] <= enc_end)].copy()\\n\",\n",
    "    \"    if len(enc_df) < enc_len:\\n\",\n",
    "    \"        # pad by repeating earliest row (simple fallback)\\n\",\n",
    "    \"        pad_n = enc_len - len(enc_df)\\n\",\n",
    "    \"        pad = pd.concat([enc_df.head(1)] * pad_n, ignore_index=True)\\n\",\n",
    "    \"        enc_df = pd.concat([pad, enc_df], ignore_index=True)\\n\",\n",
    "    \"    # build predict dataset and dataloader\\n\",\n",
    "    \"    model_input = pd.concat([enc_df, pd.DataFrame([row])], ignore_index=True)\\n\",\n",
    "    \"    model_input[TARGET] = model_input[TARGET].fillna(0.0)\\n\",\n",
    "    \"    model_input['series'] = 'cherry_blossom'\\n\",\n",
    "    \"    predict_ds = TimeSeriesDataSet.from_dataset(training, model_input, predict=True, stop_randomization=True)\\n\",\n",
    "    \"    predict_dl = predict_ds.to_dataloader(train=False, batch_size=1, num_workers=0)\\n\",\n",
    "    \"    out = model.predict(predict_dl) if hasattr(model, 'predict') else trainer.predict(model, predict_dl)\\n\",\n",
    "    \"    # normalize output extraction\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        if isinstance(out, list) or isinstance(out, tuple):\\n\",\n",
    "    \"            outv = out[0]\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            outv = out\\n\",\n",
    "    \"        if hasattr(outv, 'numpy') or hasattr(outv, 'detach'):\\n\",\n",
    "    \"            arr = np.array(outv)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            arr = np.array(outv)\\n\",\n",
    "    \"        preds.append(float(np.asarray(arr).flatten()[0]))\\n\",\n",
    "    \"    except Exception:\\n\",\n",
    "    \"        preds.append(np.nan)\\n\",\n",
    "    \"\\n\",\n",
    "    \"test_df['timegpt_pred'] = preds\\n\",\n",
    "    \"print('Done predictions; sample:')\\n\",\n",
    "    \"print(test_df[['year','bloom_day','timegpt_pred']].head())\\n\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n",
    "\n"
   ],
   "id": "4a2552c9b97215bf"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
